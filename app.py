import streamlit as st
from streamlit_geolocation import streamlit_geolocation
import pandas as pd
from math import radians, sin, cos, sqrt, atan2
import os
import re
import glob
import io # ì¶”ê°€: StringIOë¥¼ ìœ„í•´ ì„í¬íŠ¸

# Langchain ê´€ë ¨ import
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import CSVLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
load_dotenv()

st.set_page_config(page_title="ê´€ê´‘ì§€ ì¶”ì²œ ì±—ë´‡", layout="wide")

# --- íŒŒì¼ ê²½ë¡œ ì •ì˜ (ìƒìˆ˜) ---
VECTOR_DB_PATH = "faiss_tourist_attractions"

# ë¡œë“œí•  ê°œë³„ ê´€ê´‘ì§€ CSV íŒŒì¼ ëª©ë¡ì„ ì§ì ‘ ì§€ì •í•©ë‹ˆë‹¤.
# **ì—¬ê¸°ë¥¼ ì‹¤ì œ CSV íŒŒì¼ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš”!**
# Streamlit Cloudì—ì„œëŠ” ìƒëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
TOUR_CSV_FILES = [
    "./ê²½ê¸°ë„ì—­ì‚¬ê´€ê´‘ì§€í˜„í™©.csv",
    "./ê²½ê¸°ë„ìì—°ê´€ê´‘ì§€í˜„í™©.csv",
    "./ê²½ê¸°ë„ì²´í—˜ê´€ê´‘ì§€í˜„í™©.csv",
    "./ê²½ê¸°ë„í…Œë§ˆê´€ê´‘ì§€í˜„í™©.csv",
    "./ê´€ê´‘ì§€ì •ë³´í˜„í™©(ì œê³µí‘œì¤€).csv",
    "./ê´€ê´‘ì§€í˜„í™©.csv",
    # í•„ìš”ì— ë”°ë¼ ë‹¤ë¥¸ CSV íŒŒì¼ë“¤ì„ ì—¬ê¸°ì— ì¶”ê°€í•˜ì„¸ìš”.
]

# --- ì´ˆê¸° íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ---
# ëª¨ë“  í•„ìˆ˜ ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
required_files = TOUR_CSV_FILES
for f_path in required_files:
    if not os.path.exists(f_path):
        st.error(f"í•„ìˆ˜ ë°ì´í„° íŒŒì¼ '{f_path}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. (Streamlit Cloudì—ì„œëŠ” í•´ë‹¹ íŒŒì¼ë“¤ì´ Git ë¦¬í¬ì§€í† ë¦¬ì— í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.)")
        st.stop()


# --- 1. ì„¤ì • ë° ì´ˆê¸°í™” í•¨ìˆ˜ ---
def setup_environment():
    """
    í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” Streamlit secretsì—ì„œ OpenAI API í‚¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    Streamlit Cloud í™˜ê²½ì—ì„œëŠ” st.secretsë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” .env íŒŒì¼ì„ ë¡œë“œí•˜ê±°ë‚˜ ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    if 'OPENAI_API_KEY' in st.secrets:
        return st.secrets['OPENAI_API_KEY']
    else:
        load_dotenv() # ë¡œì»¬ ê°œë°œ ì‹œ .env íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            pass # ì„±ê³µ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ì§€ ì•Šë„ë¡ ë³€ê²½
        else:
            st.error("âŒ OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Streamlit Cloudì—ì„œëŠ” secrets.tomlì— í‚¤ë¥¼ ì„¤ì •í•˜ê±°ë‚˜, ë¡œì»¬ì—ì„œëŠ” .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return api_key


def initialize_streamlit_app():
    """Streamlit ì•±ì˜ ê¸°ë³¸ í˜ì´ì§€ ì„¤ì • ë° ì œëª©ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    st.title("ğŸ—ºï¸ ìœ„ì¹˜ ê¸°ë°˜ ê´€ê´‘ì§€ ì¶”ì²œ ë° ì—¬í–‰ ê³„íš ì±—ë´‡")

# --- 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜ ---
@st.cache_data
def load_specific_tour_data(file_paths_list):
    """ì§€ì •ëœ CSV íŒŒì¼ ëª©ë¡ì„ ë¡œë“œí•˜ê³ , ëª¨ë“  íŒŒì¼ì— CP949 ì¸ì½”ë”©ì„ ì ìš©í•˜ì—¬ ë³‘í•©í•©ë‹ˆë‹¤."""
    combined_df = pd.DataFrame()

    if not file_paths_list:
        st.error("ë¡œë“œí•  ê´€ê´‘ì§€ CSV íŒŒì¼ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `TOUR_CSV_FILES`ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    for file_path in file_paths_list:
        if not os.path.exists(file_path):
            st.warning(f"'{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœ±ë‹ˆë‹¤. (Streamlit Cloudì—ì„œëŠ” í•´ë‹¹ íŒŒì¼ë“¤ì´ Git ë¦¬í¬ì§€í† ë¦¬ì— í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.)")
            continue

        # ëª¨ë“  íŒŒì¼ì— CP949 ì¸ì½”ë”© ì ìš©
        current_encoding = 'cp949'

        try:
            df = pd.read_csv(file_path, encoding=current_encoding)
            df.columns = df.columns.str.strip()

            if "ìœ„ë„" not in df.columns or "ê²½ë„" not in df.columns:
                st.warning(f"'{os.path.basename(file_path)}' íŒŒì¼ì€ 'ìœ„ë„', 'ê²½ë„' ì»¬ëŸ¼ì´ ì—†ì–´ ê±´ë„ˆëœ±ë‹ˆë‹¤.")
                continue

            name_col = None
            for candidate in ["ê´€ê´‘ì§€ëª…", "ê´€ê´‘ì •ë³´ëª…","ê´€ê´‘ì§€"]:
                if candidate in df.columns:
                    name_col = candidate
                    break

            if name_col is None:
                df["ê´€ê´‘ì§€ëª…"] = "ì´ë¦„ ì—†ìŒ"
            else:
                df["ê´€ê´‘ì§€ëª…"] = df[name_col]

            address_col = None
            for candidate in ["ì •ì œë„ë¡œëª…ì£¼ì†Œ","ì •ì œì§€ë²ˆì£¼ì†Œ","ì†Œì¬ì§€ë„ë¡œëª…ì£¼ì†Œ","ì†Œì¬ì§€ì§€ë²ˆì£¼ì†Œ","ê´€ê´‘ì§€ì†Œì¬ì§€ì§€ë²ˆì£¼ì†Œ","ê´€ê´‘ì§€ì†Œì¬ì§€ë„ë¡œëª…ì£¼ì†Œ"]:
                if candidate in df.columns:
                    address_col = candidate
                    break

            if address_col is None:
                df["ì†Œì¬ì§€ë„ë¡œëª…ì£¼ì†Œ"] = "ì£¼ì†Œ ì—†ìŒ"
            else:
                df["ì†Œì¬ì§€ë„ë¡œëª…ì£¼ì†Œ"] = df[address_col]

            df = df[["ìœ„ë„", "ê²½ë„", "ê´€ê´‘ì§€ëª…", "ì†Œì¬ì§€ë„ë¡œëª…ì£¼ì†Œ"]]

            combined_df = pd.concat([combined_df, df], ignore_index=True)

        except Exception as e:
            st.warning(f"'{os.path.basename(file_path)}' íŒŒì¼ ({current_encoding} ì¸ì½”ë”© ì‹œë„) ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    if combined_df.empty:
        st.error("ì§€ì •ëœ íŒŒì¼ë“¤ì—ì„œ ìœ íš¨í•œ ê´€ê´‘ì§€ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. `TOUR_CSV_FILES`ì™€ íŒŒì¼ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    return combined_df


# --- ë²¡í„°ìŠ¤í† ì–´ ë¡œë”© ë° ìºì‹± ---
@st.cache_resource
def load_and_create_vectorstore_from_specific_files(tour_csv_files_list):
    """ì§€ì •ëœ CSV íŒŒì¼ ëª©ë¡ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    all_city_tour_docs = []
    for file_path in tour_csv_files_list:
        if not os.path.exists(file_path):
            st.warning(f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„±ì„ ìœ„í•´ '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœ±ë‹ˆë‹¤.")
            continue

        # ëª¨ë“  íŒŒì¼ì— CP949 ì¸ì½”ë”© ì ìš©
        current_encoding = 'cp949'

        try:
            city_tour_loader = CSVLoader(file_path=file_path, encoding=current_encoding, csv_args={'delimiter': ','})
            all_city_tour_docs.extend(city_tour_loader.load())
        except Exception as e:
            st.warning(f"'{os.path.basename(file_path)}' íŒŒì¼ ({current_encoding} ì¸ì½”ë”© ì‹œë„) ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë²¡í„°ìŠ¤í† ì–´): {e}")

    all_documents = all_city_tour_docs

    if not all_documents:
        st.error("ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. CSV íŒŒì¼ ê²½ë¡œì™€ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=50)
    docs = text_splitter.split_documents(all_documents)
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(docs, embeddings)
    vectorstore.save_local(VECTOR_DB_PATH)
    return vectorstore

@st.cache_resource()
def get_vectorstore_cached(tour_csv_files_list):
    """ìºì‹œëœ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
    if os.path.exists(VECTOR_DB_PATH):
        try:
            return FAISS.load_local(
                VECTOR_DB_PATH,
                OpenAIEmbeddings(),
                allow_dangerous_deserialization=True
            )
        except Exception as e:
            st.warning(f"ê¸°ì¡´ ë²¡í„° DB ë¡œë”© ì‹¤íŒ¨: {e}. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            return load_and_create_vectorstore_from_specific_files(tour_csv_files_list)
    else:
        return load_and_create_vectorstore_from_specific_files(tour_csv_files_list)


# --- Haversine distance function ---
def haversine(lat1, lon1, lat2, lon2):
    R = 6371  # Radius of Earth in kilometers
    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2
    c = 2 * atan2(sqrt(a), sqrt(1 - a))
    distance = R * c
    return distance

# --- 3. ì‚¬ìš©ì ì…ë ¥ ë° UI ë¡œì§ í•¨ìˆ˜ ---
def get_user_inputs_ui():
    """ì‚¬ìš©ìë¡œë¶€í„° ë‚˜ì´, ì—¬í–‰ ìŠ¤íƒ€ì¼, í˜„ì¬ ìœ„ì¹˜, ê·¸ë¦¬ê³  ì¶”ê°€ ì—¬í–‰ ê³„íš ì •ë³´ë¥¼ ì…ë ¥ë°›ëŠ” UIë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown("#### ì‚¬ìš©ì ì •ë³´ ì…ë ¥")
        age = st.selectbox("ë‚˜ì´ëŒ€ ì„ íƒ", ["10ëŒ€", "20ëŒ€", "30ëŒ€", "40ëŒ€", "50ëŒ€ ì´ìƒ"], key='age_selectbox')
        travel_style = st.multiselect("ì—¬í–‰ ìŠ¤íƒ€ì¼", ["ìì—°", "ì—­ì‚¬", "ì²´í—˜", "íœ´ì‹", "ë¬¸í™”", "ê°€ì¡±", "ì•¡í‹°ë¹„í‹°"], key='travel_style_multiselect')

    st.header("â‘  ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸°")
    location = streamlit_geolocation()

    user_lat_final, user_lon_final = None, None

    if location and "latitude" in location and "longitude" in location:
        temp_lat = location.get("latitude")
        temp_lon = location.get("longitude")
        if temp_lat is not None and temp_lon is not None:
            user_lat_final = temp_lat
            user_lon_final = temp_lon
            st.success(f"ğŸ“ í˜„ì¬ ìœ„ì¹˜: ìœ„ë„ {user_lat_final:.7f}, ê²½ë„ {user_lon_final:.7f}")
        else:
            st.warning("ğŸ“ ìœ„ì¹˜ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        st.warning("ìœ„ì¹˜ ì •ë³´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ìœ„ë„, ê²½ë„ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

    if user_lat_final is None or user_lon_final is None:
        default_lat = st.session_state.get("user_lat", 37.5665) # ì„œìš¸ ì‹œì²­ ê¸°ë³¸ ìœ„ë„
        default_lon = st.session_state.get("user_lon", 126.9780) # ì„œìš¸ ì‹œì²­ ê¸°ë³¸ ê²½ë„

        st.subheader("ì§ì ‘ ìœ„ì¹˜ ì…ë ¥ (ì„ íƒ ì‚¬í•­)")
        manual_lat = st.number_input("ìœ„ë„", value=float(default_lat), format="%.7f", key="manual_lat_input")
        manual_lon = st.number_input("ê²½ë„", value=float(default_lon), format="%.7f", key="manual_lon_input")

        if manual_lat != 0.0 or manual_lon != 0.0:
            user_lat_final = manual_lat
            user_lon_final = manual_lon
        else:
            user_lat_final = None
            user_lon_final = None
            st.error("ìœ íš¨í•œ ìœ„ë„ ë° ê²½ë„ ê°’ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 0ì´ ì•„ë‹Œ ê°’ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    st.session_state.user_lat = user_lat_final
    st.session_state.user_lon = user_lon_final

    st.markdown("#### ì¶”ê°€ ì—¬í–‰ ê³„íš ì •ë³´")
    trip_duration_days = st.number_input("ì—¬í–‰ ê¸°ê°„ (ì¼)", min_value=1, value=3, key='trip_duration')
    estimated_budget = st.number_input("ì˜ˆìƒ ì˜ˆì‚° (ì›, ì´ ê¸ˆì•¡)", min_value=0, value=500000, step=10000, key='estimated_budget')
    num_travelers = st.number_input("ì—¬í–‰ ì¸ì› (ëª…)", min_value=1, value=2, key='num_travelers')
    special_requests = st.text_area("íŠ¹ë³„íˆ ê³ ë ¤í•  ì‚¬í•­ (ì„ íƒ ì‚¬í•­)", help="ì˜ˆ: ìœ ëª¨ì°¨ ì‚¬ìš©, ê³ ë ¹ì ë™ë°˜, íŠ¹ì • ìŒì‹ ì„ í˜¸ ë“±", key='special_requests')

    return age, travel_style, user_lat_final, user_lon_final, trip_duration_days, estimated_budget, num_travelers, special_requests

# --- 4. ì¶”ì²œ ë¡œì§ í•¨ìˆ˜ (Langchain API ë³€ê²½: create_retrieval_chain ì‚¬ìš©) (í”„ë¡¬í”„íŠ¸ ìˆ˜ì •) ---
@st.cache_resource
def get_qa_chain(_vectorstore):
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0.7)

    # 1. ë¬¸ì„œ ì²´ì¸ (í”„ë¡¬í”„íŠ¸ + ë¬¸ì„œ ê²°í•©) ìƒì„±
    qa_prompt = PromptTemplate.from_template(
        """
ë‹¹ì‹ ì€ ì‚¬ìš©ì ìœ„ì¹˜ ê¸°ë°˜ ì—¬í–‰ì§€ ì¶”ì²œ ë° ìƒì„¸ ì—¬í–‰ ê³„íš ìˆ˜ë¦½ ì±—ë´‡ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ë‚˜ì´ëŒ€, ì—¬í–‰ ì„±í–¥, í˜„ì¬ ìœ„ì¹˜ ì •ë³´, ê·¸ë¦¬ê³  ë‹¤ìŒì˜ ì¶”ê°€ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ê´€ê´‘ì§€ë¥¼ ì¶”ì²œí•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•œ ì—¬í–‰ ê³„íšì„ ìˆ˜ë¦½í•´ ì£¼ì„¸ìš”.
**ê´€ê´‘ì§€ ì¶”ì²œ ì‹œ ì‚¬ìš©ì ìœ„ì¹˜ë¡œë¶€í„°ì˜ ê±°ë¦¬ëŠ” ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ ì¶”ê°€í•  ê²ƒì´ë¯€ë¡œ, ë‹µë³€ì—ì„œ ê±°ë¦¬ë¥¼ ì§ì ‘ ì–¸ê¸‰í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.**
íŠ¹íˆ, ì‚¬ìš©ìì˜ í˜„ì¬ ìœ„ì¹˜({user_lat}, {user_lon})ì—ì„œ ê°€ê¹Œìš´ ì¥ì†Œë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ì¶”ì²œí•˜ê³  ê³„íšì„ ì„¸ì›Œì£¼ì„¸ìš”.
ê¼­ê¼­ ì‚¬ìš©ì í˜„ì¬ ìœ„ì¹˜ì™€ ê°€ê¹Œìš´ ê³³ì„ ìµœìš°ì„ ìœ¼ë¡œ í•´ì£¼ê³  ì‚¬ìš©ìê°€ ì„ íƒí•œ ì„±í–¥ì— ë§ê²Œ ì¶”ì²œí•´ì£¼ì„¸ìš”.

[ê´€ê´‘ì§€ ë°ì´í„°]
{context}

[ì‚¬ìš©ì ì •ë³´]
ë‚˜ì´ëŒ€: {age}
ì—¬í–‰ ì„±í–¥: {travel_style}
í˜„ì¬ ìœ„ì¹˜ (ìœ„ë„, ê²½ë„): {user_lat}, {user_lon}
ì—¬í–‰ ê¸°ê°„: {trip_duration_days}ì¼
ì˜ˆìƒ ì˜ˆì‚°: {estimated_budget}ì›
ì—¬í–‰ ì¸ì›: {num_travelers}ëª…
íŠ¹ë³„ ê³ ë ¤ì‚¬í•­: {special_requests}

[ì‚¬ìš©ì ì§ˆë¬¸]
{input}

ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ ìƒì„¸í•œ ì—¬í–‰ ê³„íšì„ ì„¸ì›Œì£¼ì„¸ìš”:
1.  **ê´€ê´‘ì§€ ì¶”ì²œ:** ì§ˆë¬¸ì— ë¶€í•©í•˜ê³ , ì‚¬ìš©ì ìœ„ì¹˜ì—ì„œ ê°€ê¹Œìš´ 1~3ê°œì˜ ì£¼ìš” ê´€ê´‘ì§€ë¥¼ ì¶”ì²œí•˜ê³ , ê° ê´€ê´‘ì§€ì— ëŒ€í•œ ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
    * ê´€ê´‘ì§€ ì´ë¦„: [ê´€ê´‘ì§€ëª…]
    * ì£¼ì†Œ: [ì£¼ì†Œ]
    * ì£¼ìš” ì‹œì„¤/íŠ¹ì§•: [ì •ë³´]
    **[ì°¸ê³ : ì‚¬ìš©ì ìœ„ì¹˜ ê¸°ì¤€ ê±°ë¦¬ëŠ” ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ ì¶”ê°€í•  ê²ƒì´ë¯€ë¡œ, ì´ í•­ëª©ì€ ì œì™¸í•©ë‹ˆë‹¤.]**
    
2.  **ì¶”ì²œëœ ê´€ê´‘ì§€ë¥¼ í¬í•¨í•˜ì—¬, ì‚¬ìš©ì ì •ë³´ì™€ ì§ˆë¬¸ì— ê¸°ë°˜í•œ {trip_duration_days}ì¼ê°„ì˜ ìƒì„¸ ì—¬í–‰ ê³„íšì„ ì¼ìë³„ë¡œ êµ¬ì„±í•´ ì£¼ì„¸ìš”.**
    * ê° ë‚ ì§œë³„ë¡œ ë°©ë¬¸í•  ì¥ì†Œ(ì‹ë‹¹, ì¹´í˜, ê¸°íƒ€ í™œë™ í¬í•¨), ì˜ˆìƒ ì‹œê°„, ê°„ë‹¨í•œ í™œë™ ë‚´ìš©ì„ í¬í•¨í•˜ì„¸ìš”.
    * ì˜ˆì‚°ì„ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ ì‹ì‚¬ ì¥ì†Œë‚˜ í™œë™ì„ ì œì•ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    * ì´ë™ ê²½ë¡œ(ì˜ˆ: "ë„ë³´ 15ë¶„", "ë²„ìŠ¤ 30ë¶„")ë¥¼ ê°„ëµí•˜ê²Œ ì–¸ê¸‰í•´ ì£¼ì„¸ìš”.
    * ê³„íšì€ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

[ë‹µë³€ ì˜ˆì‹œ]
**ì¶”ì²œ ê´€ê´‘ì§€:**
- ê´€ê´‘ì§€ ì´ë¦„: [ê´€ê´‘ì§€ëª… 1]
  - ì£¼ì†Œ: [ì£¼ì†Œ 1]
  - ì£¼ìš” ì‹œì„¤/íŠ¹ì§•: [ì •ë³´ 1]
- ê´€ê´‘ì§€ ì´ë¦„: [ê´€ê´‘ì§€ëª… 2]
  - ì£¼ì†Œ: [ì£¼ì†Œ 2]
  - ì£¼ìš” ì‹œì„¤/íŠ¹ì§•: [ì •ë³´ 2]

**ìƒì„¸ ì—¬í–‰ ê³„íš ({trip_duration_days}ì¼):**
ë‹¤ìŒ í‘œ í˜•ì‹ìœ¼ë¡œ ì¼ìë³„ ìƒì„¸ ê³„íšì„ ì‘ì„±í•´ ì£¼ì„¸ìš”. ì»¬ëŸ¼ëª…ì€ 'ì¼ì°¨', 'ì‹œê°„', 'í™œë™', 'ì˜ˆìƒ ì¥ì†Œ', 'ì´ë™ ë°©ë²•'ìœ¼ë¡œ í•´ì£¼ì„¸ìš”.
| ì¼ì°¨ | ì‹œê°„ | í™œë™ | ì˜ˆìƒ ì¥ì†Œ | ì´ë™ ë°©ë²• |
|---|---|---|---|---|
| 1ì¼ì°¨ | ì˜¤ì „ (9:00 - 12:00) | [í™œë™ ë‚´ìš©] | [ì¥ì†Œëª…] | [ì´ë™ ë°©ë²•] |
| 1ì¼ì°¨ | ì ì‹¬ (12:00 - 13:00) | [ì‹ì‚¬] | [ì‹ë‹¹ëª…] | - |
| 1ì¼ì°¨ | ì˜¤í›„ (13:00 - 17:00) | [í™œë™ ë‚´ìš©] | [ì¥ì†Œëª…] | [ì´ë™ ë°©ë²•] |
| 1ì¼ì°¨ | ì €ë… (17:00 ì´í›„) | [í™œë™ ë‚´ìš©] | [ì¥ì†Œëª… ë˜ëŠ” ììœ  ì‹œê°„] | - |
| 2ì¼ì°¨ | ... | ... | ... | ... |
**ì¤‘ìš”: 'ì¼ì°¨' ì»¬ëŸ¼ì˜ ê²½ìš°, ê°™ì€ ì¼ì°¨ì˜ ì—¬ëŸ¬ í™œë™ì´ ìˆì„ ê²½ìš° ì²« ë²ˆì§¸ í™œë™ì—ë§Œ í•´ë‹¹ 'ì¼ì°¨'ë¥¼ ëª…ì‹œí•˜ê³ , ë‚˜ë¨¸ì§€ í™œë™ í–‰ì˜ 'ì¼ì°¨' ì…€ì€ ë¹„ì›Œë‘ì„¸ìš” (ì˜ˆ: "| | ì‹œê°„ | í™œë™ | ì˜ˆìƒ ì¥ì†Œ | ì´ë™ ë°©ë²• |"). ì´ë ‡ê²Œ í•´ì•¼ í‘œì—ì„œ 'ì¼ì°¨'ê°€ ìë™ìœ¼ë¡œ ë³‘í•©ë˜ì–´ ë³´ì…ë‹ˆë‹¤.**
"""
    )
    document_chain = create_stuff_documents_chain(llm, qa_prompt)
    retriever = _vectorstore.as_retriever(search_kwargs={"k": 15})
    retrieval_chain = create_retrieval_chain(retriever, document_chain)

    return retrieval_chain


# --- 5. ë©”ì¸ ì•± ì‹¤í–‰ ë¡œì§ ---
if __name__ == "__main__":
    openai_api_key = setup_environment()
    if not openai_api_key:
        st.stop()

    initialize_streamlit_app()

    vectorstore = get_vectorstore_cached(TOUR_CSV_FILES)

    # --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë° ì´ì „ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ ---
    if "conversations" not in st.session_state or "messages" in st.session_state:
        st.session_state.conversations = []
        if "messages" in st.session_state:
            del st.session_state.messages
        st.session_state.current_input = ""
        st.session_state.selected_conversation_index = None

    qa_chain = get_qa_chain(vectorstore)
    tour_data_df = load_specific_tour_data(TOUR_CSV_FILES)

    # Sidebar for previous conversations
    with st.sidebar:
        st.subheader("ğŸ’¡ ì´ì „ ëŒ€í™”")
        if st.session_state.conversations:
            for i, conv in enumerate(reversed(st.session_state.conversations)):
                original_index = len(st.session_state.conversations) - 1 - i
                
                if 'travel_style_selected' in conv and conv['travel_style_selected'] and conv['travel_style_selected'] != 'íŠ¹ì • ì—†ìŒ':
                    preview_text = f"ì„±í–¥: {conv['travel_style_selected']}"
                    if len(preview_text) > 25: 
                        preview_text = preview_text[:22] + '...'
                else:
                    preview_text = conv['user_query'][:25] + ('...' if len(conv['user_query']) > 25 else '')
                    
                if st.button(f"ëŒ€í™” {original_index + 1}: {preview_text}", key=f"sidebar_conv_{original_index}"):
                    st.session_state.selected_conversation_index = original_index
                    st.rerun()

        else:
            st.info("ì´ì „ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # --- ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ ---
    if st.session_state.selected_conversation_index is not None:
        st.header("ğŸ“– ì„ íƒëœ ì´ì „ ëŒ€í™” ë‚´ìš©")
        
        selected_conv = st.session_state.conversations[st.session_state.selected_conversation_index]
        
        st.subheader("ğŸ™‹â€â™‚ï¸ ì‚¬ìš©ì ì§ˆë¬¸:")
        st.markdown(selected_conv['user_query'])
        
        if 'travel_style_selected' in selected_conv and selected_conv['travel_style_selected'] and selected_conv['travel_style_selected'] != 'íŠ¹ì • ì—†ìŒ':
            st.subheader("âœ¨ ì„ íƒëœ ì—¬í–‰ ì„±í–¥:")
            st.markdown(selected_conv['travel_style_selected'])

        st.subheader("ğŸ¤– ì±—ë´‡ ë‹µë³€:")
        # ì´ì „ ëŒ€í™”ëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. (í‘œë¡œ íŒŒì‹±í•˜ì§€ ì•ŠìŒ)
        st.markdown(selected_conv['chatbot_response'])
        
        st.markdown("---")
        if st.button("ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘í•˜ê¸°"):
            st.session_state.selected_conversation_index = None
            st.session_state.current_input = ""
            st.rerun()

    else: # ì´ì „ ëŒ€í™”ê°€ ì„ íƒë˜ì§€ ì•Šì€ ê²½ìš° (ìƒˆë¡œìš´ ì§ˆë¬¸ ì…ë ¥ ìƒíƒœ)
        age, travel_style_list, current_user_lat, current_user_lon, \
        trip_duration_days, estimated_budget, num_travelers, special_requests = get_user_inputs_ui()

        st.header("â‘¡ ì§ˆë¬¸í•˜ê¸°")
        user_query = st.text_input("ì–´ë–¤ ì—¬í–‰ì„ ê³„íší•˜ê³  ê³„ì‹ ê°€ìš”? (ì˜ˆ: ê°€ì¡±ê³¼ í•¨ê»˜ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ìì—° í…Œë§ˆ ì—¬í–‰)", value=st.session_state.current_input, key="user_input")

        if st.button("ì—¬í–‰ ê³„íš ì¶”ì²œë°›ê¸°"):
            st.session_state.selected_conversation_index = None 

            lat_to_invoke = current_user_lat
            lon_to_invoke = current_user_lon

            age_to_invoke = age
            travel_style_to_invoke = ', '.join(travel_style_list) if travel_style_list else 'íŠ¹ì • ì—†ìŒ'
            trip_duration_days_to_invoke = trip_duration_days
            estimated_budget_to_invoke = estimated_budget
            num_travelers_to_invoke = num_travelers
            special_requests_to_invoke = special_requests

            if lat_to_invoke is None or lon_to_invoke is None:
                st.warning("ìœ„ì¹˜ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìœ„ì¹˜ ì •ë³´ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ê°€ì ¸ì™€ ì£¼ì„¸ìš”.")
            elif not user_query.strip():
                st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                with st.spinner("ìµœì ì˜ ì—¬í–‰ ê³„íšì„ ìˆ˜ë¦½ ì¤‘ì…ë‹ˆë‹¤..."):
                    try:
                        response = qa_chain.invoke({
                            "input": user_query,
                            "age": age_to_invoke,
                            "travel_style": travel_style_to_invoke,
                            "user_lat": lat_to_invoke,
                            "user_lon": lon_to_invoke,
                            "trip_duration_days": trip_duration_days_to_invoke,
                            "estimated_budget": estimated_budget_to_invoke,
                            "num_travelers": num_travelers_to_invoke,
                            "special_requests": special_requests_to_invoke
                        })

                        rag_result_text = response["answer"]

                        processed_output_lines = []
                        processed_place_names = set()
                        table_plan_text = ""
                        in_plan_section = False # ì—¬í–‰ ê³„íš ì„¹ì…˜ì¸ì§€ í™•ì¸í•˜ëŠ” í”Œë˜ê·¸

                        # LLM ì‘ë‹µì—ì„œ ê´€ê´‘ì§€ ì •ë³´ ì¶”ì¶œ ë° ê±°ë¦¬ ì¶”ê°€ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                        for line in rag_result_text.split('\n'):
                            if "ìƒì„¸ ì—¬í–‰ ê³„íš" in line and "ì¼ì°¨ | ì‹œê°„ | í™œë™" not in line:
                                processed_output_lines.append(line)
                                in_plan_section = True
                                continue 

                            if not in_plan_section:
                                name_match = re.search(r"ê´€ê´‘ì§€ ì´ë¦„:\s*(.+)", line)
                                if name_match:
                                    current_place_name = name_match.group(1).strip()
                                    if current_place_name not in processed_place_names:
                                        processed_output_lines.append(line)
                                        processed_place_names.add(current_place_name)

                                        found_place_data = tour_data_df[
                                            (tour_data_df['ê´€ê´‘ì§€ëª…'].str.strip() == current_place_name) &
                                            (pd.notna(tour_data_df['ìœ„ë„'])) &
                                            (pd.notna(tour_data_df['ê²½ë„']))
                                        ]
                                        if not found_place_data.empty:
                                            place_lat = found_place_data['ìœ„ë„'].iloc[0]
                                            place_lon = found_place_data['ê²½ë„'].iloc[0]
                                            distance = haversine(lat_to_invoke, lon_to_invoke, place_lat, place_lon)
                                            processed_output_lines.append(f"- ì‚¬ìš©ì ìœ„ì¹˜ ê¸°ì¤€ ê±°ë¦¬(km): ì•½ {distance:.2f} km")
                                        else:
                                            processed_output_lines.append("- ì‚¬ìš©ì ìœ„ì¹˜ ê¸°ì¤€ ê±°ë¦¬(km): ì •ë³´ ì—†ìŒ (ë°ì´í„° ë¶ˆì¼ì¹˜ ë˜ëŠ” ì¢Œí‘œ ëˆ„ë½)")
                                else:
                                    if not re.search(r"ê±°ë¦¬\(km\):", line):
                                        processed_output_lines.append(line)
                            else:
                                # ì—¬í–‰ ê³„íš ì„¹ì…˜ì˜ ë¼ì¸ë“¤ì„ ë³„ë„ë¡œ ì €ì¥ (í‘œ íŒŒì‹±ìš©)
                                table_plan_text += line + "\n"

                        # ì¶”ì²œ ê´€ê´‘ì§€ ë° ì¼ë°˜ì ì¸ ì •ë³´ ë¨¼ì € í‘œì‹œ
                        st.subheader("âœ… ì¶”ì²œ ê²°ê³¼ ë° ìƒì„¸ ì—¬í–‰ ê³„íš")
                        st.markdown("\n".join(processed_output_lines))

                        # ì—¬í–‰ ê³„íš í…Œì´ë¸” íŒŒì‹± ë° í‘œì‹œ
                        if table_plan_text.strip():
                            try:
                                plan_lines = table_plan_text.strip().split('\n')
                                
                                # Markdown í…Œì´ë¸”ì˜ í—¤ë”ì™€ êµ¬ë¶„ì ë¼ì¸ ê²€ì‚¬
                                if len(plan_lines) >= 2 and plan_lines[0].count('|') >= 2 and plan_lines[1].count('|') >= 2 and all(re.match(r'^-+$', s.strip()) for s in plan_lines[1].split('|') if s.strip()):
                                    header = [h.strip() for h in plan_lines[0].split('|') if h.strip()]
                                    data_rows = []
                                    for row_str in plan_lines[2:]:
                                        if row_str.strip() and row_str.startswith('|'):
                                            # ê° ì…€ì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
                                            # ë‹¨, ë¹ˆ ì…€ì€ ê·¸ëŒ€ë¡œ ë¹ˆ ë¬¸ìì—´ë¡œ ìœ ì§€
                                            parsed_row = [d.strip() for d in row_str.split('|')]
                                            # ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ë¹ˆ ë¬¸ìì—´ ì œê±° (split ê²°ê³¼)
                                            if parsed_row and parsed_row[0] == '':
                                                parsed_row = parsed_row[1:]
                                            if parsed_row and parsed_row[-1] == '':
                                                parsed_row = parsed_row[:-1]
                                            data_rows.append(parsed_row)

                                    if data_rows:
                                        # í—¤ë”ì™€ ë°ì´í„° ì»¬ëŸ¼ ìˆ˜ê°€ ë‹¤ë¥¼ ê²½ìš° ì—ëŸ¬ ë°©ì§€
                                        if all(len(row) == len(header) for row in data_rows):
                                            temp_plan_df = pd.DataFrame(data_rows, columns=header)
                                            
                                            # --- í•µì‹¬ ë³€ê²½: 'ì¼ì°¨' ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •í•˜ê³ , ì¤‘ë³µ ì¸ë±ìŠ¤ ìˆ¨ê¸°ê¸° ---
                                            if 'ì¼ì°¨' in temp_plan_df.columns:
                                                # ` ì¼ì°¨ ` ì»¬ëŸ¼ì˜ ì—°ì†ì ì¸ ì¤‘ë³µ ê°’ì„ NaNìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ì‹œê°ì ìœ¼ë¡œ ìˆ¨ê¹€
                                                for i in range(1, len(temp_plan_df)):
                                                    if temp_plan_df.loc[i, 'ì¼ì°¨'] == temp_plan_df.loc[i-1, 'ì¼ì°¨']:
                                                        temp_plan_df.loc[i, 'ì¼ì°¨'] = '' # ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •í•˜ì—¬ ìˆ¨ê¹€
                                                
                                                # 'ì¼ì°¨' ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
                                                # (ì£¼ì˜: set_indexëŠ” ë³µì‚¬ë³¸ì„ ë°˜í™˜í•˜ë¯€ë¡œ ë‹¤ì‹œ í• ë‹¹í•´ì•¼ í•¨)
                                                plan_df_styled = temp_plan_df.set_index('ì¼ì°¨')
                                                
                                                st.subheader("ğŸ—“ï¸ ìƒì„¸ ì—¬í–‰ ê³„íš (í‘œ)")
                                                # st.dataframeì— DataFrame Styler ì‚¬ìš©
                                                st.dataframe(plan_df_styled, use_container_width=True)
                                            else:
                                                st.subheader("ğŸ—“ï¸ ìƒì„¸ ì—¬í–‰ ê³„íš (í‘œ)")
                                                st.dataframe(temp_plan_df, use_container_width=True)
                                                st.warning("ì—¬í–‰ ê³„íšì— 'ì¼ì°¨' ì»¬ëŸ¼ì´ ì—†ì–´ ê·¸ë£¹í™”í•˜ì—¬ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                            # --- í•µì‹¬ ë³€ê²½ ë ---
                                        else:
                                            st.warning("ì—¬í–‰ ê³„íš í…Œì´ë¸”ì˜ í–‰ê³¼ ì—´ì˜ ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•Šì•„ í‘œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. LLM ì‘ë‹µ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                                    else:
                                        st.warning("ì—¬í–‰ ê³„íš í…Œì´ë¸” ë‚´ìš©ì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. LLMì´ ìš”ì²­ëœ í‘œ í˜•ì‹ì„ ë”°ë¥´ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                                else:
                                    st.warning("ì—¬í–‰ ê³„íšì´ ìœ íš¨í•œ í‘œ í˜•ì‹ìœ¼ë¡œ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                            except Exception as parse_e:
                                st.error(f"ì—¬í–‰ ê³„íš í…Œì´ë¸” íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {parse_e}. LLM ì‘ë‹µ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                        else:
                            st.info("ìƒì„¸ ì—¬í–‰ ê³„íšì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        
                        # ìƒˆë¡œìš´ ëŒ€í™” ìŒì„ ì €ì¥í•©ë‹ˆë‹¤.
                        st.session_state.conversations.append({
                            "user_query": user_query,
                            "chatbot_response": rag_result_text, # ì›ë³¸ LLM ì‘ë‹µì„ ì €ì¥
                            "travel_style_selected": travel_style_to_invoke
                        })

                    except ValueError as ve:
                        st.error(f"ì²´ì¸ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {ve}. ì…ë ¥ í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                    except Exception as e:
                        st.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")

                st.session_state.current_input = "" # ì…ë ¥ì°½ ì´ˆê¸°í™”
